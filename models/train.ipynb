{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requirimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gcsfs datasets\n",
    "!pip install --upgrade sentence-transformers\n",
    "!pip install --upgrade transformers\n",
    "!pip install unidecode\n",
    "!python -m spacy download pt_core_news_md\n",
    "!pip install --upgrade torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Fixar semente para reprodução\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Carregar os dados\n",
    "csv_path = \"/content/dataset_transformado.csv\"  # Atualize com o caminho correto\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Codificar as classes\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"Label\"] = label_encoder.fit_transform(data[\"Label\"])\n",
    "\n",
    "# Calcular pesos das classes\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=data[\"Label\"].unique(),\n",
    "    y=data[\"Label\"]\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "train_texts, eval_texts, train_labels, eval_labels = train_test_split(\n",
    "    data[\"Texto\"], data[\"Label\"], test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizar os textos\n",
    "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-large-portuguese-cased\")\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
    "eval_encodings = tokenizer(list(eval_texts), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Criar datasets compatíveis com o Trainer\n",
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = ClassificationDataset(train_encodings, list(train_labels))\n",
    "eval_dataset = ClassificationDataset(eval_encodings, list(eval_labels))\n",
    "\n",
    "# Carregar o modelo para classificação\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"neuralmind/bert-large-portuguese-cased\",\n",
    "    num_labels=len(label_encoder.classes_),  # Número de classes\n",
    ")\n",
    "\n",
    "# Configurar os argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/bert-ptbr-classification\",\n",
    "    num_train_epochs=4,  # Aumentado para explorar mais o treinamento\n",
    "    per_device_train_batch_size=16,  # Reduzido para evitar problemas de memória\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    learning_rate=2e-5,  # Taxa de aprendizado ajustada\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"/content/logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=452,  # Avaliações mais frequentes\n",
    "    save_steps=452,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    run_name=\"bert-ptbr-classification\",\n",
    ")\n",
    "\n",
    "# Definir a métrica de avaliação\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=1).numpy()\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "# Criar o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Adicionado callback para early stopping\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "trainer.train()\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "output_dir = f\"models/bert-ptbr-classification-final\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O gato subiu na árvore e miou alto.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizer salvos\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Diretório onde o modelo foi salvo\n",
    "output_dir = \"models/bert-ptbr-classification-final\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Frase para teste\n",
    "test_phrases = [\n",
    "\n",
    "    \n",
    "    \"loja de reparo de celular perto\"\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenizar as frases de teste\n",
    "inputs = tokenizer(test_phrases, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Fazer a predição\n",
    "model.eval()  # Colocar o modelo em modo de avaliação\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    print(logits)\n",
    "    predictions = torch.argmax(logits, dim=1).numpy()\n",
    "    print(f' predicitions --->{predictions}')\n",
    "\n",
    "# Decodificar os índices das classes previstas\n",
    "predicted_classes = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "# Exibir as frases e suas respectivas previsões\n",
    "for phrase, pred_class in zip(test_phrases, predicted_classes):\n",
    "    print(f\"Frase: '{phrase}' -> Classe prevista: '{pred_class} prob {predictions}'\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "values, indices = torch.topk(probabilities, 10, dim=1)\n",
    "rounded_values = [round(value,2) for value in values[0].tolist()]\n",
    "\n",
    "print(\"Probabilidades dos 10 maiores:\",rounded_values )  \n",
    "print(\"Índices dos 10 maiores:\", label_encoder.inverse_transform(indices[0].tolist()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
