{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requirimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gcsfs datasets\n",
    "!pip install --upgrade sentence-transformers\n",
    "!pip install --upgrade transformers\n",
    "!pip install unidecode\n",
    "!python -m spacy download pt_core_news_md\n",
    "!pip install --upgrade torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Fixar semente para reprodução\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Carregar os dados\n",
    "csv_path = \"/content/dataset_transformado_not_lema.csv\"  # Atualize com o caminho correto\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Codificar as classes\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"Label\"] = label_encoder.fit_transform(data[\"Label\"])\n",
    "\n",
    "# Salvar o mapeamento label -> id no modelo\n",
    "id2label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "label2id = {label: i for i, label in enumerate(label_encoder.classes_)}\n",
    "\n",
    "# Calcular pesos das classes\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=data[\"Label\"].unique(),\n",
    "    y=data[\"Label\"]\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "train_texts, eval_texts, train_labels, eval_labels = train_test_split(\n",
    "    data[\"Texto\"], data[\"Label\"], test_size=0.1, stratify=data[\"Label\"], random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizar os textos\n",
    "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-large-portuguese-cased\")\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
    "eval_encodings = tokenizer(list(eval_texts), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Criar datasets compatíveis com o Trainer\n",
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = ClassificationDataset(train_encodings, list(train_labels))\n",
    "eval_dataset = ClassificationDataset(eval_encodings, list(eval_labels))\n",
    "\n",
    "# Carregar o modelo para classificação\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"neuralmind/bert-large-portuguese-cased\",\n",
    "    num_labels=len(label_encoder.classes_),  # Número de classes\n",
    ")\n",
    "\n",
    "# Adicionar label2id e id2label no config do modelo\n",
    "model.config.label2id = label2id\n",
    "model.config.id2label = id2label\n",
    "\n",
    "# Configurar os argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/bert-ptbr-classification\",\n",
    "    num_train_epochs=5,  # Aumentado para explorar mais o treinamento\n",
    "    per_device_train_batch_size=32,  # Reduzido para evitar problemas de memória\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    learning_rate=2e-5,  # Taxa de aprendizado ajustada\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"/content/logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=397,\n",
    "    save_steps=397,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    run_name=\"bert-ptbr-classification\",\n",
    ")\n",
    "\n",
    "# Definir a métrica de avaliação\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=1).numpy()\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "# Criar o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Adicionado callback para early stopping\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "trainer.train()\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "output_dir = f\"models/bert-ptbr-classification-final\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O gato subiu na árvore e miou alto.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Diretório onde o modelo foi salvo\n",
    "output_dir = \"/content/models/bert-ptbr-classification-final\"\n",
    "\n",
    "# Carregar o modelo e o tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Frases para teste\n",
    "test_phrases = [\n",
    "    \"quero uma honda civic\"\n",
    "]\n",
    "\n",
    "# Tokenizar as frases de teste\n",
    "inputs = tokenizer(test_phrases, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Fazer a predição\n",
    "model.eval()  # Colocar o modelo em modo de avaliação\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).numpy()\n",
    "\n",
    "# Decodificar os índices das classes previstas usando id2label\n",
    "predicted_classes = [model.config.id2label[pred] for pred in predictions]\n",
    "\n",
    "# Exibir as frases e suas respectivas previsões\n",
    "for phrase, pred_class in zip(test_phrases, predicted_classes):\n",
    "    print(f\"Frase: '{phrase}' -> Classe prevista: '{pred_class}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "# Obter as 10 maiores probabilidades e seus índices\n",
    "values, indices = torch.topk(probabilities, 30, dim=1)\n",
    "rounded_values = [round(value.item(), 2) for value in values[0]]\n",
    "\n",
    "# Usar o id2label para mapear índices para rótulos\n",
    "id2label = model.config.id2label\n",
    "top_labels = [id2label[idx.item()] for idx in indices[0]]\n",
    "\n",
    "print(\"Probabilidades dos 10 maiores:\", rounded_values)\n",
    "print(\"Índices dos 10 maiores:\", top_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Preparar os dados de teste para avaliação\n",
    "test_texts, test_labels = eval_texts.tolist(), eval_labels.tolist()\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)\n",
    "test_dataset = ClassificationDataset(test_encodings, test_labels)\n",
    "\n",
    "# Realizar predições no conjunto de teste\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_logits = predictions.predictions\n",
    "pred_labels = np.argmax(pred_logits, axis=1)\n",
    "\n",
    "# Calcular a matriz de confusão e o relatório de classificação\n",
    "conf_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "class_report = classification_report(test_labels, pred_labels, target_names=label_encoder.classes_)\n",
    "\n",
    "# Exibir a matriz de confusão\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Exibir o relatório de classificação\n",
    "print(\"\\nRelatório de Classificação (Taxa de Acerto por Classe):\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Calcular a matriz de confusão\n",
    "conf_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "\n",
    "# Calcular a precisão do modelo\n",
    "accuracy = accuracy_score(test_labels, pred_labels)\n",
    "\n",
    "# Plotar a matriz de confusão com a precisão no título\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(f\"Confusion Matrix\\nAccuracy: {accuracy:.2%}\")\n",
    "plt.show()\n",
    "\n",
    "# Salvar a matriz de confusão como arquivo\n",
    "plt.savefig(\"confusion_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('iniciando ..')\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "#login()\n",
    "\n",
    "# Caminho do modelo local\n",
    "output_dir = \"/content/models/bert-ptbr-classification-final\"\n",
    "\n",
    "# Adicionar mapeamento de labels\n",
    "model.config.label2id = label2id\n",
    "model.config.id2label = id2label\n",
    "\n",
    "# Salvar modelo e tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Subir para o Hugging Face Hub\n",
    "model.push_to_hub(\"SenhorDasMoscas/acho-classification-14-01-2025\")\n",
    "tokenizer.push_to_hub(\"SenhorDasMoscas/acho-classification-14-01-2025\")\n",
    "\n",
    "# Carregar do Hugging Face Hub\n",
    "model = BertForSequenceClassification.from_pretrained(\"SenhorDasMoscas/acho-classification-14-01-2025\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"SenhorDasMoscas/acho-classification-14-01-2025\")\n",
    "\n",
    "# Testar\n",
    "test_phrases = [\"quero ajeitar o ar do meu carro\"]\n",
    "inputs = tokenizer(test_phrases, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).numpy()\n",
    "\n",
    "# Decodificar\n",
    "predicted_classes = [model.config.id2label[pred] for pred in predictions]\n",
    "for phrase, pred_class in zip(test_phrases, predicted_classes):\n",
    "    print(f\"Frase: '{phrase}' -> Classe prevista: '{pred_class}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
